{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x250f6229670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = []\n",
    "for root, dirs, files in os.walk(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            name = file\n",
    "            name = name.replace(\".jpg\", '')\n",
    "            name = name.split('_')[:-1]\n",
    "            name = ' '.join(name)\n",
    "            filename_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_name = []\n",
    "for item in filename_list:\n",
    "    if item not in unique_name:\n",
    "        unique_name.append(item)\n",
    "dog_list = []\n",
    "cat_list = []\n",
    "for item in unique_name:\n",
    "    if item[0].isupper():\n",
    "        cat_list.append(item)\n",
    "    else:\n",
    "        dog_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##format data in folder\n",
    "#import shutilfor root, dirs, files in os.walk(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images\"):\n",
    "#\n",
    "#        r = np.random.rand()\n",
    "#    for file in files:\n",
    "#            if r > 0.92:\n",
    "#        if file[0].isupper():\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/test/' + 'cat')\n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, \n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, \n",
    "#            elif r <= 0.72:\n",
    "#            else:\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/train/' + 'cat')\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/val/' + 'cat')\n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, \n",
    "#            if r > 0.92:\n",
    "#        else:\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/test/' + 'dog')\n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, \n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, \n",
    "#            elif r <= 0.72:\n",
    "#            else:\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/train/' + 'dog')\n",
    "#                            'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/val/' + 'dog')\n",
    "#                shutil.move(\"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images/\" + file, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = \"C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories/val/cat\"\n",
    "#for root, dirs, files in os.walk(dir):\n",
    "#    for file in files:\n",
    "#        shutil.move(dir + '/' + file, 'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod data\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = 'C:/Users/Annika Wong/OneDrive/KTH/2022-2023Spring_P4/DD2424/project/oxford-iiit-pet/data_2_categories'\n",
    "train_datesets = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train'])\n",
    "train_loaders = torch.utils.data.DataLoader(train_datesets, batch_size=4, shuffle=True, num_workers=4)\n",
    "train_dataset_sizes = len(train_datesets)\n",
    "\n",
    "val_datesets = datasets.ImageFolder(os.path.join(data_dir, 'val'), data_transforms['val'])\n",
    "val_loaders = torch.utils.data.DataLoader(val_datesets, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_dataset_sizes = len(val_datesets)\n",
    "\n",
    "test_datesets = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "test_loaders = torch.utils.data.DataLoader(test_datesets, batch_size=4, shuffle=True, num_workers=4)\n",
    "test_dataset_sizes = len(test_datesets)\n",
    "\n",
    "class_name = train_datesets.classes\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#from torchvision.utils import make_grid\n",
    "#\n",
    "#def show_images(images, nmax=64):\n",
    "#    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#    ax.set_xticks([]); ax.set_yticks([])\n",
    "#    ax.imshow(make_grid((images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "#def show_batch(dl, nmax=64):\n",
    "#    for images in dl:\n",
    "#        show_images(images, nmax)\n",
    "#        break\n",
    "#\n",
    "#show_batch(test_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_para = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Current epoch: {epoch}')\n",
    "\n",
    "        ###################### TRAINING #######################\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        temp_count = 0\n",
    "        for batch, label in train_loaders:\n",
    "            if temp_count % 100 == 0:\n",
    "                print(f'Current batch {temp_count}')\n",
    "            temp_count += 1\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # Forward\n",
    "                output = model(batch)\n",
    "\n",
    "                _, preds = torch.max(output, 1)\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "                #Backward\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch.size(0)\n",
    "            accuracy += torch.sum(preds == label.data)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss/train_dataset_sizes\n",
    "        epoch_accuracy = accuracy/train_dataset_sizes\n",
    "        print(f'Total train image {temp_count}')\n",
    "        print(f'Training Loss: {epoch_loss} Accuracy: {epoch_accuracy}')\n",
    "\n",
    "        ###################### VALIDATION #######################\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        temp_count = 0\n",
    "\n",
    "        for batch, label in val_loaders:\n",
    "            if temp_count % 100 == 0:\n",
    "                print(f'Current image {temp_count}')\n",
    "            temp_count += 1\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                #Forward\n",
    "                output = model(batch)\n",
    "                _, preds = torch.max(output, 1)\n",
    "                loss = criterion(output, label)\n",
    "            \n",
    "            running_loss += loss.item() * batch.size(0)\n",
    "            accuracy += torch.sum(preds == label.data)\n",
    "\n",
    "        epoch_loss = running_loss/val_dataset_sizes\n",
    "        epoch_accuracy = accuracy/val_dataset_sizes\n",
    "\n",
    "        print(f'Total val image {temp_count}')\n",
    "        print(f'Validation Loss: {epoch_loss} Accuracy: {epoch_accuracy}')\n",
    "\n",
    "        if epoch_accuracy > best_accuracy:\n",
    "            best_accuracy = epoch_accuracy\n",
    "            best_model_para = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_para)\n",
    "\n",
    "    ###################### TEST #######################\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for batch, label in test_loaders:\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        #Forward\n",
    "        output = model(batch)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        running_loss += loss.item() * batch.size(0)\n",
    "        accuracy += torch.sum(preds == label.data)\n",
    "\n",
    "    test_loss = running_loss/test_dataset_sizes\n",
    "    test_accuracy = accuracy/test_dataset_sizes\n",
    "\n",
    "    print(f'Test Loss: {test_loss} Accuracy: {test_accuracy}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_features = model_tuned.fc.in_features\n",
    "\n",
    "output_size = 2\n",
    "model_tuned.fc = nn.Linear(num_features, output_size) #cat and dog\n",
    "\n",
    "model_tuned = model_tuned.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_model = optim.Adam(model_tuned.parameters())\n",
    "\n",
    "lr_scheduler_model = lr_scheduler.StepLR(optimizer_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.9565999647304275 Accuracy: 0.6116760969161987\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.3688893690142907 Accuracy: 0.6300897002220154\n",
      "Test Loss: 1.2323078618882293 Accuracy: 0.6244056820869446\n"
     ]
    }
   ],
   "source": [
    "model_tuned = training(model_tuned, criterion, optimizer_model, lr_scheduler_model, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 0\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.9561071458860193 Accuracy: 0.6233521699905396\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.319199910549366 Accuracy: 0.6293995976448059\n",
      "Current epoch: 1\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.9635523935383548 Accuracy: 0.6169491410255432\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.4208392315486984 Accuracy: 0.623878538608551\n",
      "Current epoch: 2\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.9403976331491255 Accuracy: 0.6177024245262146\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.30016574030967 Accuracy: 0.6114561557769775\n",
      "Current epoch: 3\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.951641369716259 Accuracy: 0.6145009398460388\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.4378106670348374 Accuracy: 0.6273291707038879\n",
      "Current epoch: 4\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n",
      "Current batch 1100\n",
      "Current batch 1200\n",
      "Current batch 1300\n",
      "Total train image 1328\n",
      "Training Loss: 0.950690139675769 Accuracy: 0.6184557676315308\n",
      "Current image 0\n",
      "Current image 100\n",
      "Current image 200\n",
      "Current image 300\n",
      "Total val image 363\n",
      "Validation Loss: 1.2545221593125555 Accuracy: 0.6293995976448059\n",
      "Current epoch: 5\n",
      "Current batch 0\n",
      "Current batch 100\n",
      "Current batch 200\n",
      "Current batch 300\n",
      "Current batch 400\n",
      "Current batch 500\n",
      "Current batch 600\n",
      "Current batch 700\n",
      "Current batch 800\n",
      "Current batch 900\n",
      "Current batch 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANNIKA~1\\AppData\\Local\\Temp/ipykernel_25556/3639007205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_tuned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_tuned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ANNIKA~1\\AppData\\Local\\Temp/ipykernel_25556/303561514.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Annika Wong\\Anaconda\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Annika Wong\\Anaconda\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Annika Wong\\Anaconda\\Anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Annika Wong\\Anaconda\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Annika Wong\\Anaconda\\Anaconda\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_tuned = training(model_tuned, criterion, optimizer_model, lr_scheduler_model, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lambda_ = np.linspace(0.0002, 0.0006, 5)\n",
    "eta_max = np.linspace(1e-1, 5e-1, 5)\n",
    "cycles = np.linspace(3,7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 4. 5. 6. 7.]\n",
      "[0.1 0.2 0.3 0.4 0.5]\n",
      "[0.0002 0.0003 0.0004 0.0005 0.0006]\n"
     ]
    }
   ],
   "source": [
    "print(cycles)\n",
    "print(eta_max)\n",
    "print(lambda_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
